{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0220f5-a2b9-4db0-8fff-4a8af48e261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import csv\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"LOG: Using device: {device}\")\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "\n",
    "try:\n",
    "    model_name = \".\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name).to(device)\n",
    "    print(f\"LOG: BERT Model ('{model_name}') and Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load BERT model ('{model_name}'). {e}\")\n",
    "    exit(1)\n",
    "\n",
    "def get_embedding(text):\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(map(str, text))\n",
    "        \n",
    "    if not text or text.strip().lower() == \"none\":\n",
    "        return torch.zeros(768, device=device)\n",
    "\n",
    "    try:\n",
    "        tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "        return outputs.last_hidden_state[:, 0, :].squeeze().to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to generate embedding for text: {text[:50]}... {e}\")\n",
    "        return torch.zeros(768, device=device)\n",
    "\n",
    "input_file = os.path.join(DATA_DIR, \"./clean_data/YOUR_FILE_NAME_HERE.json\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"ERROR: Dataset file not found at {input_file}\")\n",
    "    exit(1)\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "print(f\"LOG: Loaded {len(dataset)} records from dataset\")\n",
    "\n",
    "output_dir = \"./data/temp_json\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_node = []\n",
    "model_index = []\n",
    "model_features = {}\n",
    "model_features_with_nodes = {}\n",
    "target_node = []\n",
    "target_index = []\n",
    "edge_index = []\n",
    "total_edge_index = []\n",
    "edge_attr = []\n",
    "target_features = {}\n",
    "edge_weight_with_nodes = {}\n",
    "target_edges = []\n",
    "target_edges_with_weights = {}\n",
    "model_dict = {}\n",
    "reverse_model_dict = {}\n",
    "target_dict = {}\n",
    "reverse_target_dict = {}\n",
    "edges_dict = {}\n",
    "count_model = {}\n",
    "count_target = {}\n",
    "target_edge_dict = {}\n",
    "\n",
    "batch_size = 100\n",
    "batch_edge_index = []\n",
    "batch_edge_attr = []\n",
    "batch_target_edge_index = []\n",
    "batch_target_edge_attr = []\n",
    "batch_count = 0\n",
    "\n",
    "for index, data in enumerate(dataset):\n",
    "    paper_id = data.get(\"id\", f\"paper_{index}\")\n",
    "    if (index + 1) % 100 == 0:\n",
    "        print(f\"LOG: Processing record {index + 1}/{len(dataset)}\")\n",
    "        print(f\"LOG: Current model_id count: {len(model_dict)}\")\n",
    "        print(f\"LOG: Current target_id count: {len(target_dict)}\")\n",
    "        print(f\"LOG: Current edge count: {len(edges_dict)}\")\n",
    "        print(f\"LOG: Current target_edge count: {len(target_edges)}\")\n",
    "\n",
    "        top_models = sorted(count_model.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"LOG: Top 5 most frequent model_ids:\")\n",
    "        for model_id, count in top_models:\n",
    "            model_main_text = reverse_model_dict.get(model_id, \"Unknown\")\n",
    "            print(f\"  - {model_id} ({model_main_text}): {count} occurrences\")\n",
    "\n",
    "        top_targets = sorted(count_target.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"LOG: Top 5 most frequent target_ids:\")\n",
    "        for target_id, count in top_targets:\n",
    "            target_text = reverse_target_dict.get(target_id, \"Unknown\")\n",
    "            print(f\"  - {target_id} ({target_text}): {count} occurrences\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    " \n",
    "    model_main_text = data.get(\"model_main\", \"\").strip().replace(\"none\", \"\")\n",
    "    if not model_main_text:\n",
    "        print(f\"WARNING: Skipping entry with empty model_main\")\n",
    "        continue\n",
    "\n",
    "    if model_main_text not in model_dict:\n",
    "        model_id = len(model_dict)\n",
    "        model_dict[model_main_text] = model_id\n",
    "        reverse_model_dict[model_id] = model_main_text\n",
    "        count_model[model_id] = 1\n",
    "    else:\n",
    "        model_id = model_dict[model_main_text]\n",
    "        count_model[model_id] += 1\n",
    "\n",
    "    model_main_embedding = get_embedding(model_main_text)\n",
    "\n",
    "    def rodrigues_rotation(vector, angle_deg, axis):\n",
    "        device = vector.device\n",
    "        angle_rad = torch.tensor(np.radians(angle_deg), dtype=torch.float32, device=device)\n",
    "    \n",
    "        axis = axis.to(device) / torch.norm(axis.to(device))\n",
    "        identity = torch.eye(vector.shape[0], device=device)\n",
    "        axis_outer = torch.ger(axis, axis).to(device)\n",
    "\n",
    "        cos_theta = torch.cos(angle_rad)\n",
    "        sin_theta = torch.sin(angle_rad)\n",
    "\n",
    "        rotation_matrix = cos_theta * identity + (1 - cos_theta) * axis_outer + sin_theta * torch.diag(axis)\n",
    "        rotated_vector = torch.matmul(rotation_matrix, vector)\n",
    "\n",
    "        return rotated_vector\n",
    "\n",
    "    common_rotation_axis = torch.ones(768)\n",
    "    common_rotation_axis = common_rotation_axis / torch.norm(common_rotation_axis)\n",
    "\n",
    "    model_features_text = \" \".join([\n",
    "        data.get(\"species\", \"\"), data.get(\"age\", \"\"), data.get(\"sex\", \"\"),\n",
    "        data.get(\"biosample_main\", \"\"), data.get(\"biosample_detail\", \"\"),\n",
    "        data.get(\"experiment_type\", \"\"), data.get(\"model_main\", \"\"),\n",
    "        data.get(\"model_detail1\", \"\"), data.get(\"model_detail2\", \"\"),\n",
    "        data.get(\"model_detail3\", \"\"), data.get(\"timepoint\", \"\")\n",
    "    ]).strip().replace(\"none\", \"\")\n",
    "\n",
    "    model_features_embedding = get_embedding(model_features_text)\n",
    "\n",
    "    species = data.get(\"species\", \"\").strip().lower()\n",
    "    species_rotation_params = {\n",
    "        \"human\": (174.20, 2.5152),\n",
    "        \"mouse\": (176.36, 5.7104),\n",
    "        \"rat\": (173.81, 5.7201)\n",
    "    }\n",
    "\n",
    "    if species in species_rotation_params:\n",
    "        angle, norm_factor = species_rotation_params[species]\n",
    "        rotation_axis = common_rotation_axis.to(model_features_embedding.device)\n",
    "    \n",
    "        model_features_embedding = rodrigues_rotation(model_features_embedding, angle, rotation_axis)\n",
    "        model_features_embedding = model_features_embedding * norm_factor\n",
    "\n",
    "    model_node.append(model_main_embedding)\n",
    "    model_index.append(model_id)\n",
    "    \n",
    "    if model_id not in model_features:\n",
    "        model_features[model_id] = []\n",
    "\n",
    "    model_features[model_id].append(model_features_embedding)\n",
    "   \n",
    "    if model_id not in model_features_with_nodes:\n",
    "        model_features_with_nodes[model_id] = {\"model\": model_main_embedding, \"features\": []}\n",
    "\n",
    "    model_features_with_nodes[model_id][\"features\"].append(model_features_embedding)\n",
    "    targets = data.get(\"targets\", [])\n",
    "\n",
    "    if isinstance(targets, dict):\n",
    "        print(f\"WARNING: `targets` is a dictionary! Converting to list: {targets}\")\n",
    "        targets = [targets]\n",
    "\n",
    "    if not isinstance(targets, list):\n",
    "        print(f\"ERROR: Unexpected format for `targets`: {targets}\")\n",
    "        targets = []\n",
    "\n",
    "    target_ids = []\n",
    "    edge_index = []\n",
    "    edge_attr = [] \n",
    "    \n",
    "    for target in targets:\n",
    "        target_text = target.get(\"target\", \"\").strip().replace(\"none\", \"\")\n",
    "\n",
    "        if not target_text:\n",
    "            print(f\"WARNING: Skipping target with empty name -> {target}\")\n",
    "            continue\n",
    "\n",
    "        if target_text not in target_dict:\n",
    "            target_id = len(target_dict)\n",
    "            target_dict[target_text] = target_id\n",
    "            reverse_target_dict[target_id] = target_text\n",
    "            count_target[target_id] = 1\n",
    "        else:\n",
    "            target_id = target_dict[target_text]\n",
    "            count_target[target_id] += 1\n",
    "\n",
    "        target_ids.append(target_id)\n",
    "        target_embedding = get_embedding(target_text)\n",
    "\n",
    "        if isinstance(target, dict):\n",
    "            target = [target]\n",
    "    \n",
    "        if isinstance(target, list):\n",
    "            target_node.append(target_embedding)\n",
    "        else:\n",
    "            print(f\"WARNING: Unexpected type for target: {type(target)}. Skipping append.\")\n",
    "\n",
    "        edge_weight_text = \" \".join([\n",
    "            target[0].get(\"target\", \"\"), target[0].get(\"molecule_type\", \"\"),\n",
    "            target[0].get(\"analysis_main\", \"\"), target[0].get(\"analysis_detail\", \"\"),\n",
    "            target[0].get(\"relation\", \"\"), target[0].get(\"change\", \"\"),\n",
    "            target[0].get(\"significance\", \"\"), target[0].get(\"control\", \"\")\n",
    "        ]).strip().replace(\"none\", \"\")\n",
    "\n",
    "        relation = target[0].get(\"relation\", \"\").strip().lower()\n",
    "        edge_weight_embedding = get_embedding(edge_weight_text)\n",
    "\n",
    "        if relation == \"increase\":\n",
    "            edge_weight_embedding = rodrigues_rotation(edge_weight_embedding, 90, common_rotation_axis)\n",
    "        elif relation == \"decrease\":\n",
    "            edge_weight_embedding = rodrigues_rotation(edge_weight_embedding, -90, common_rotation_axis)\n",
    "\n",
    "        if target_id not in target_features:\n",
    "            target_features[target_id] = []\n",
    "\n",
    "        target_features[target_id].append(edge_weight_embedding)\n",
    "\n",
    "        if target_id not in edge_weight_with_nodes:\n",
    "            edge_weight_with_nodes[target_id] = {\"target\": target_embedding, \"edge_weight\": []} \n",
    "\n",
    "        edge_weight_with_nodes[target_id][\"edge_weight\"].append(edge_weight_embedding)\n",
    "\n",
    "        if (model_id, target_id) not in edges_dict:\n",
    "            edges_dict[(model_id, target_id)] = []\n",
    "\n",
    "        edge_feature = torch.cat((model_features_embedding, edge_weight_embedding), dim=0)\n",
    "\n",
    "        new_edge_entry = {\n",
    "            \"model_features\": model_features_embedding,\n",
    "            \"edge_weight\": edge_weight_embedding\n",
    "        }\n",
    "        edges_dict[(model_id, target_id)].append(new_edge_entry)\n",
    "\n",
    "        edge_index.append([model_id, target_id])\n",
    "        total_edge_index.append([model_id, target_id])\n",
    "        edge_attr.append(edge_feature)\n",
    "\n",
    "    target_edge_index = []\n",
    "    target_edge_attr = []\n",
    "    target_edges_with_weights = {}\n",
    "\n",
    "    for i in range(len(target_ids)):\n",
    "        for j in range(i + 1, len(target_ids)):\n",
    "            target_id_i, target_id_j = target_ids[i], target_ids[j]\n",
    "\n",
    "            if (target_id_i, target_id_j) in target_edges_with_weights or (target_id_j, target_id_i) in target_edges_with_weights:\n",
    "                continue\n",
    "\n",
    "            edge_info_i = edges_dict.get((model_id, target_id_i), [])\n",
    "            edge_info_j = edges_dict.get((model_id, target_id_j), [])\n",
    "\n",
    "            if isinstance(edge_info_i, list) and len(edge_info_i) > 0:\n",
    "                edge_info_i = edge_info_i[-1]\n",
    "\n",
    "            if isinstance(edge_info_j, list) and len(edge_info_j) > 0:\n",
    "                edge_info_j = edge_info_j[-1]\n",
    "\n",
    "            if isinstance(edge_info_i, dict) and isinstance(edge_info_j, dict):\n",
    "                edge_weights_i = edge_info_i[\"edge_weight\"]\n",
    "                edge_weights_j = edge_info_j[\"edge_weight\"]\n",
    "            else:\n",
    "                print(f\"WARNING: Skipping edge between {target_id_i} and {target_id_j} due to missing edge_weight.\")\n",
    "                continue\n",
    "\n",
    "            target_edges.append((target_id_i, target_id_j))\n",
    "            target_edges.append((target_id_j, target_id_i))\n",
    "\n",
    "            if (target_id_i, target_id_j) not in target_edges_with_weights:\n",
    "                target_edges_with_weights[(target_id_i, target_id_j)] = []\n",
    "            if (target_id_j, target_id_i) not in target_edges_with_weights:\n",
    "                target_edges_with_weights[(target_id_j, target_id_i)] = []\n",
    "\n",
    "            target_edges_with_weights[(target_id_i, target_id_j)].append({\n",
    "                \"edge_weight_source\": edge_weights_i,\n",
    "                \"edge_weight_target\": edge_weights_j\n",
    "            })\n",
    "            target_edges_with_weights[(target_id_j, target_id_i)].append({\n",
    "                \"edge_weight_source\": edge_weights_j,\n",
    "                \"edge_weight_target\": edge_weights_i\n",
    "            })\n",
    "\n",
    "            if (target_id_i, target_id_j) not in target_edge_dict:\n",
    "                target_edge_dict[(target_id_i, target_id_j)] = []\n",
    "            if (target_id_j, target_id_i) not in target_edge_dict:\n",
    "                target_edge_dict[(target_id_j, target_id_i)] = []\n",
    "\n",
    "            target_edge_dict[(target_id_i, target_id_j)].append([edge_weights_i, edge_weights_j])\n",
    "            target_edge_dict[(target_id_j, target_id_i)].append([edge_weights_j, edge_weights_i])\n",
    "\n",
    "    for (target_id_i, target_id_j), weight_list in target_edges_with_weights.items():\n",
    "            for weight in weight_list:\n",
    "                target_edge_index.append([target_id_i, target_id_j])\n",
    "                target_edge_attr.append(torch.cat((weight[\"edge_weight_source\"], weight[\"edge_weight_target\"]), dim=0))\n",
    "\n",
    "    batch_edge_index.extend(edge_index)\n",
    "    batch_edge_attr.extend(edge_attr)\n",
    "    batch_target_edge_index.extend(target_edge_index)\n",
    "    batch_target_edge_attr.extend(target_edge_attr)\n",
    "    batch_count += 1\n",
    "\n",
    "    if batch_count >= batch_size:\n",
    "        batch_id = (index + 1) // batch_size\n",
    "        json_files = {\n",
    "            \"edge_index\": os.path.join(output_dir, f\"batch_{batch_id}_edge_index.json\"),\n",
    "            \"edge_attr\": os.path.join(output_dir, f\"batch_{batch_id}_edge_attr.json\"),\n",
    "            \"target_edge_index\": os.path.join(output_dir, f\"batch_{batch_id}_target_edge_index.json\"),\n",
    "            \"target_edge_attr\": os.path.join(output_dir, f\"batch_{batch_id}_target_edge_attr.json\"),\n",
    "        }\n",
    "\n",
    "        with open(json_files[\"edge_index\"], \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(batch_edge_index, f, indent=4)\n",
    "        with open(json_files[\"edge_attr\"], \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([t.tolist() for t in batch_edge_attr], f, indent=4)\n",
    "        with open(json_files[\"target_edge_index\"], \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(batch_target_edge_index, f, indent=4)\n",
    "        with open(json_files[\"target_edge_attr\"], \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([t.tolist() for t in batch_target_edge_attr], f, indent=4)\n",
    "\n",
    "        print(f\"ðŸ“„ Batch {batch_id} has been saved\")\n",
    "        batch_edge_index.clear()\n",
    "        batch_edge_attr.clear()\n",
    "        batch_target_edge_index.clear()\n",
    "        batch_target_edge_attr.clear()\n",
    "        batch_count = 0\n",
    "\n",
    "if batch_count > 0:\n",
    "    batch_id += 1\n",
    "    json_files = {\n",
    "        \"edge_index\": os.path.join(output_dir, f\"batch_{batch_id}_edge_index.json\"),\n",
    "        \"edge_attr\": os.path.join(output_dir, f\"batch_{batch_id}_edge_attr.json\"),\n",
    "        \"target_edge_index\": os.path.join(output_dir, f\"batch_{batch_id}_target_edge_index.json\"),\n",
    "        \"target_edge_attr\": os.path.join(output_dir, f\"batch_{batch_id}_target_edge_attr.json\"),\n",
    "    }\n",
    "\n",
    "    with open(json_files[\"edge_index\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(batch_edge_index, f, indent=4)\n",
    "    with open(json_files[\"edge_attr\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([t.tolist() for t in batch_edge_attr], f, indent=4)\n",
    "    with open(json_files[\"target_edge_index\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(batch_target_edge_index, f, indent=4)\n",
    "    with open(json_files[\"target_edge_attr\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([t.tolist() for t in batch_target_edge_attr], f, indent=4)\n",
    "\n",
    "    print(f\"ðŸ“„ Final batch {batch_id} has been saved\")\n",
    "\n",
    "    batch_edge_index.clear()\n",
    "    batch_edge_attr.clear()\n",
    "    batch_target_edge_index.clear()\n",
    "    batch_target_edge_attr.clear()\n",
    "    batch_count = 0\n",
    "\n",
    "if index == len(dataset) - 1:\n",
    "    print(f\"LOG: Processing record {index + 1}/{len(dataset)}\")\n",
    "    print(f\"LOG: Current model_id count: {len(model_dict)}\")\n",
    "    print(f\"LOG: Current target_id count: {len(target_dict)}\")\n",
    "    print(f\"LOG: Current edge count: {len(total_edge_index)}\")\n",
    "    print(f\"LOG: Current target_edge count: {len(target_edges)}\")\n",
    "    print(f\"LOG: Current target_edge_dict count: {len(target_edge_dict)}\")\n",
    "\n",
    "    top_models = sorted(count_model.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"LOG: Top 10 most frequent model_ids:\")\n",
    "    for model_id, count in top_models:\n",
    "        model_main_text = reverse_model_dict.get(model_id, \"Unknown\")\n",
    "        print(f\"  - {model_id} ({model_main_text}): {count} occurrences\")\n",
    "\n",
    "    top_targets = sorted(count_target.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"LOG: Top 10 most frequent target_ids:\")\n",
    "    for target_id, count in top_targets:\n",
    "        target_text = reverse_target_dict.get(target_id, \"Unknown\")\n",
    "        print(f\"  - {target_id} ({target_text}): {count} occurrences\")\n",
    "\n",
    "model_features_with_nodes_file = os.path.join(output_dir, \"model_features_with_nodes.json\")\n",
    "edge_weight_with_nodes_file = os.path.join(output_dir, \"edge_weight_with_nodes.json\")\n",
    "\n",
    "with open(model_features_with_nodes_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {str(k): {\n",
    "            \"model\": v[\"model\"].tolist() if isinstance(v[\"model\"], torch.Tensor) else v[\"model\"], \n",
    "            \"features\": [f.tolist() if isinstance(f, torch.Tensor) else f for f in v[\"features\"]]\n",
    "        }\n",
    "        for k, v in model_features_with_nodes.items()},\n",
    "        f, indent=4\n",
    "    )\n",
    "\n",
    "with open(edge_weight_with_nodes_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {str(k): {  \n",
    "            \"target\": v[\"target\"].tolist() if isinstance(v[\"target\"], torch.Tensor) else v[\"target\"], \n",
    "            \"edge_weight\": [ew.tolist() if isinstance(ew, torch.Tensor) else ew for ew in v[\"edge_weight\"]]\n",
    "        }\n",
    "        for k, v in edge_weight_with_nodes.items()},\n",
    "        f, indent=4\n",
    "    )\n",
    "\n",
    "model_features_file = os.path.join(output_dir, \"model_features.json\")\n",
    "target_features_file = os.path.join(output_dir, \"target_features.json\")\n",
    "\n",
    "with open(model_features_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {key: [tensor.tolist() for tensor in value]  # å¤‰æ›´ç‚¹: value[\"features\"] â†’ value\n",
    "         for key, value in model_features.items()}, \n",
    "        f, ensure_ascii=False, indent=4\n",
    "    )\n",
    "\n",
    "with open(target_features_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {key: [tensor.tolist() for tensor in value]  # å¤‰æ›´ç‚¹: value[\"features\"] â†’ value\n",
    "         for key, value in target_features.items()}, \n",
    "        f, ensure_ascii=False, indent=4\n",
    "    )\n",
    "\n",
    "target_edge_dict_file = os.path.join(output_dir, \"target_edge_dict.json\")\n",
    "\n",
    "target_edge_dict_serializable = {\n",
    "    str(key): [[tensor_i.tolist(), tensor_j.tolist()] for tensor_i, tensor_j in value]\n",
    "    for key, value in target_edge_dict.items()\n",
    "}\n",
    "\n",
    "with open(target_edge_dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target_edge_dict_serializable, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "model_dict_file = os.path.join(output_dir, \"model_dict.json\")\n",
    "target_dict_file = os.path.join(output_dir, \"target_dict.json\")\n",
    "\n",
    "with open(model_dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(model_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(target_dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "reverse_model_dict_file = os.path.join(output_dir, \"reverse_model_dict.json\")\n",
    "reverse_target_dict_file = os.path.join(output_dir, \"reverse_target_dict.json\")\n",
    "\n",
    "with open(reverse_model_dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reverse_model_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(reverse_target_dict_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reverse_target_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "model_output_file = os.path.join(output_dir, \"model_id_frequency.csv\")\n",
    "sorted_models = sorted(count_model.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(model_output_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Model ID\", \"Model Main Text\", \"Count\"])\n",
    "    for model_id, count in sorted_models:\n",
    "        model_main_text = reverse_model_dict.get(model_id, \"Unknown\")\n",
    "        writer.writerow([model_id, model_main_text, count])\n",
    "\n",
    "target_output_file = os.path.join(output_dir, \"target_id_frequency.csv\")\n",
    "sorted_targets = sorted(count_target.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "with open(target_output_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Target ID\", \"Target Text\", \"Count\"])\n",
    "    for target_id, count in sorted_targets:\n",
    "        target_text = reverse_target_dict.get(target_id, \"Unknown\")\n",
    "        writer.writerow([target_id, target_text, count])\n",
    "\n",
    "merged_files = {\n",
    "    \"edge_index\": os.path.join(output_dir, \"edge_index.json\"),\n",
    "    \"edge_attr\": os.path.join(output_dir, \"edge_attr.json\"),\n",
    "    \"target_edge_index\": os.path.join(output_dir, \"target_edge_index.json\"),\n",
    "    \"target_edge_attr\": os.path.join(output_dir, \"target_edge_attr.json\")\n",
    "}\n",
    "\n",
    "merged_data = {\n",
    "    \"edge_index\": [],\n",
    "    \"edge_attr\": [],\n",
    "    \"target_edge_index\": [],\n",
    "    \"target_edge_attr\": []\n",
    "}\n",
    "\n",
    "for file_name in os.listdir(output_dir):\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    if not os.path.isfile(file_path) or not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    if file_name.startswith(\"batch_\") and \"_edge_index.json\" in file_name and \"target_edge_index.json\" not in file_name:\n",
    "        key = \"edge_index\"\n",
    "    elif file_name.startswith(\"batch_\") and \"_edge_attr.json\" in file_name and \"target_edge_attr.json\" not in file_name:\n",
    "        key = \"edge_attr\"\n",
    "    elif file_name.startswith(\"batch_\") and \"_target_edge_index.json\" in file_name:\n",
    "        key = \"target_edge_index\"\n",
    "    elif file_name.startswith(\"batch_\") and \"_target_edge_attr.json\" in file_name:\n",
    "        key = \"target_edge_attr\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "        if key in [\"edge_index\", \"edge_attr\", \"target_edge_index\", \"target_edge_attr\"]:\n",
    "            merged_data[key].extend(data)\n",
    "\n",
    "for key, file_path in merged_files.items():\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged_data[key], f, indent=4)\n",
    "\n",
    "print(\"LOG: Data processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
